'use client';

import React, { createContext, useContext, useState, useRef, useCallback, useEffect } from 'react';
import { GeminiWebSocketService } from '@/services/gemini-websocket';
import { AudioProcessingService } from '@/services/audio-processing';

interface GeminiMessage {
  id: string;
  role: 'user' | 'assistant';
  content: {
    text?: string;
    audio?: {
      data: string;
      format: string;
    };
  };
  timestamp: number;
  metadata?: {
    tokens?: number;
    finishReason?: string;
  };
}

interface AIAssistantContextType {
  connectionStatus: 'disconnected' | 'connecting' | 'connected' | 'error';
  sessionId: string | null;
  isRecording: boolean;
  audioLevel: number;
  microphonePermission: 'pending' | 'granted' | 'denied';
  messages: GeminiMessage[];
  isStreaming: boolean;
  currentResponse: string;
  isTextMode: boolean;
  isSidebarOpen: boolean;
  selectedFilter: string;
  isLive: boolean;
  lastResponse: string;
  connect: () => Promise<void>;
  disconnect: () => void;
  startVoiceRecording: () => Promise<void>;
  stopVoiceRecording: () => void;
  sendTextMessage: (message: string) => void;
  setIsTextMode: (value: boolean) => void;
  setIsSidebarOpen: (value: boolean) => void;
  setSelectedFilter: (value: string) => void;
  toggleTextMode: () => void;
  toggleSidebar: () => void;
  closeSidebar: () => void;
  backToVoice: () => void;
  sendMessage: (message: string) => void;
}

const AIAssistantContext = createContext<AIAssistantContextType | undefined>(undefined);

export function useAIAssistant() {
  const context = useContext(AIAssistantContext);
  if (context === undefined) {
    throw new Error('useAIAssistant must be used within an AIAssistantProvider');
  }
  return context;
}

export function AIAssistantProvider({ children }: { children: React.ReactNode }) {
  const [connectionStatus, setConnectionStatus] = useState<'disconnected' | 'connecting' | 'connected' | 'error'>('disconnected');
  const [sessionId, setSessionId] = useState<string | null>(null);
  const [isRecording, setIsRecording] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);
  const [microphonePermission, setMicrophonePermission] = useState<'pending' | 'granted' | 'denied'>('pending');
  const [messages, setMessages] = useState<GeminiMessage[]>([]);
  const [isStreaming, setIsStreaming] = useState(false);
  const [currentResponse, setCurrentResponse] = useState('');
  const [isTextMode, setIsTextMode] = useState(false);
  const [isSidebarOpen, setIsSidebarOpen] = useState(false);
  const [selectedFilter, setSelectedFilter] = useState('Conversation');
  const [isLive, setIsLive] = useState(true);
  const [lastResponse, setLastResponse] = useState('');
  
  const wsService = useRef<GeminiWebSocketService>();
  const audioService = useRef<AudioProcessingService>();
  
  useEffect(() => {
    wsService.current = new GeminiWebSocketService(
      handleWebSocketMessage,
      handleWebSocketError,
      setConnectionStatus
    );
    
    audioService.current = new AudioProcessingService(
      handleAudioChunk,
      setAudioLevel,
      handleAudioError
    );
    
    return () => {
      wsService.current?.disconnect();
      audioService.current?.stopRecording();
    };
  }, []);

  const handleWebSocketMessage = useCallback((message: unknown) => {
    console.log('ðŸ“¨ Received message from Gemini:', message);
    
    try {
      if (message && typeof message === 'object') {
        // Handle setup complete
        if ('setupComplete' in message) {
          console.log('âœ… Setup complete');
          return;
        }
        
        // Handle server content (AI responses)
        if ('serverContent' in message) {
          const serverMessage = message as { 
            serverContent: { 
              modelTurn?: { parts?: unknown[] }; 
              turnComplete?: boolean;
              generationComplete?: boolean;
              interrupted?: boolean;
              inputTranscription?: { text: string };
              outputTranscription?: { text: string };
            } 
          };
          
          const content = serverMessage.serverContent;
          
          // Handle input transcription
          if (content.inputTranscription) {
            console.log('ðŸŽ¤ Input transcription:', content.inputTranscription.text);
          }
          
          // Handle output transcription
          if (content.outputTranscription) {
            console.log('ðŸ”Š Output transcription:', content.outputTranscription.text);
          }
          
          // Handle interruption
          if (content.interrupted) {
            console.log('âš ï¸ Response interrupted');
            setIsStreaming(false);
            setCurrentResponse('');
          }
          
          // Handle model response
          if (content.modelTurn?.parts) {
            const textPart = content.modelTurn.parts.find((part: unknown) => 
              part && typeof part === 'object' && 'text' in part
            );
            const audioPart = content.modelTurn.parts.find((part: unknown) => 
              part && typeof part === 'object' && 'inlineData' in part && 
              (part as { inlineData: { mimeType?: string } }).inlineData?.mimeType === 'audio/pcm'
            );
            
            if (textPart && typeof textPart === 'object' && 'text' in textPart) {
              const text = (textPart as { text: string }).text;
              console.log('ðŸ’¬ Received text response:', text.substring(0, 100) + '...');
              
              if (content.turnComplete || content.generationComplete) {
                console.log('âœ… Turn/Generation complete - adding to message history');
                const newMessage: GeminiMessage = {
                  id: Date.now().toString(),
                  role: 'assistant',
                  content: { text },
                  timestamp: Date.now()
                };
                setMessages(prev => [...prev, newMessage]);
                setLastResponse(text);
                setCurrentResponse('');
                setIsStreaming(false);
              } else {
                console.log('ðŸ”„ Streaming response...');
                setCurrentResponse(text);
                setIsStreaming(true);
              }
            }
            
            if (audioPart && typeof audioPart === 'object' && 'inlineData' in audioPart) {
              const data = (audioPart as { inlineData: { data: string } }).inlineData.data;
              console.log('ðŸ”Š Received audio response, playing...');
              playAudioResponse(data);
            }
          }
        }
        
        // Handle tool calls (if any)
        if ('toolCall' in message) {
          console.log('ðŸ”§ Tool call received:', message);
        }
        
        // Handle usage metadata
        if ('usageMetadata' in message) {
          const usage = (message as { usageMetadata: unknown }).usageMetadata;
          console.log('ðŸ“Š Usage metadata:', usage);
        }
      }
    } catch (error) {
      console.error('âŒ Error processing WebSocket message:', error);
    }
  }, []);

  const handleWebSocketError = useCallback((error: Error) => {
    console.error('âŒ WebSocket error:', error);
    setConnectionStatus('error');
    
    // Stop any ongoing recording
    if (isRecording) {
      audioService.current?.stopRecording();
      setIsRecording(false);
    }
  }, [isRecording]);

  const handleAudioChunk = useCallback((chunk: string) => {
    if (wsService.current && connectionStatus === 'connected') {
      console.log('ðŸŽ¤ Sending audio chunk to Gemini');
      wsService.current.sendRealtimeInput({
        audio: { data: chunk }
      });
    }
  }, [connectionStatus]);

  const handleAudioError = useCallback((error: Error) => {
    console.error('Audio error:', error);
    setMicrophonePermission('denied');
  }, []);

  const connect = useCallback(async () => {
    try {
      if (connectionStatus === 'connecting' || connectionStatus === 'connected') {
        console.log('âš ï¸ Already connecting/connected, skipping...');
        return;
      }
      
      console.log('ðŸ”— Starting Gemini Live API connection...');
      const apiKey = process.env.NEXT_PUBLIC_GOOGLE_AI_API_KEY;
      if (!apiKey) {
        console.error('âŒ API key not found in environment');
        setConnectionStatus('error');
        throw new Error('API key not configured. Please add NEXT_PUBLIC_GOOGLE_AI_API_KEY to your .env.local file.');
      }
      
      console.log('ðŸ”Œ Connecting to Gemini Live WebSocket...');
      await wsService.current?.connect(apiKey);
      
      // Wait a moment for connection to stabilize
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Setup configuration according to technical specifications
      const setupConfig = {
        model: "gemini-2.5-flash-exp-native-audio-thinking-dialog",
        generationConfig: {
          responseModalities: ["TEXT", "AUDIO"],
          speechConfig: {
            voiceConfig: {
              prebuiltVoiceConfig: {
                voiceName: "Aoede"
              }
            }
          },
          temperature: 0.7,
          maxOutputTokens: 8192
        },
        systemInstruction: {
          parts: [{
            text: "You are a helpful AI assistant for MVP Blocks, a component library. Help users with UI components, design patterns, and development questions. Keep responses concise and practical."
          }]
        },
        realtimeInputConfig: {
          automaticActivityDetection: {
            disabled: false,
            startOfSpeechSensitivity: "START_SENSITIVITY_HIGH",
            endOfSpeechSensitivity: "END_SENSITIVITY_HIGH",
            prefixPaddingMs: 500,
            silenceDurationMs: 1000
          },
          activityHandling: "START_OF_ACTIVITY_INTERRUPTS"
        }
      };
      
      console.log('ðŸ› ï¸ Sending setup config to Gemini...');
      wsService.current?.sendSetup(setupConfig);
      
      // Create session
      try {
        console.log('ðŸŽ¯ Creating session...');
        const response = await fetch('/api/gemini/session', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ action: 'create' })
        });
        
        if (!response.ok) {
          throw new Error(`Session creation failed: ${response.status}`);
        }
        
        const { sessionId: newSessionId } = await response.json();
        console.log('âœ… Session created:', newSessionId);
        setSessionId(newSessionId);
      } catch (sessionError) {
        console.warn('âš ï¸ Session creation failed, continuing without session:', sessionError);
        // Continue without session - not critical for basic functionality
      }
      
      console.log('âœ… Gemini connection completed successfully');
      
    } catch (error) {
      console.error('âŒ Connection failed:', error);
      setConnectionStatus('error');
      throw error;
    }
  }, [connectionStatus]);

  const disconnect = useCallback(() => {
    wsService.current?.disconnect();
    audioService.current?.stopRecording();
    setIsRecording(false);
    setSessionId(null);
  }, []);

  const startVoiceRecording = useCallback(async () => {
    try {
      console.log('ðŸŽ¤ Starting voice recording...');
      if (connectionStatus !== 'connected') {
        console.log('ðŸ”„ Connection not ready, connecting first...');
        await connect();
      }
      
      console.log('ðŸŽ¤ Requesting microphone permission...');
      const hasPermission = await audioService.current?.requestMicrophonePermission();
      if (!hasPermission) {
        console.error('âŒ Microphone permission denied');
        setMicrophonePermission('denied');
        return;
      }
      
      console.log('âœ… Microphone permission granted');
      setMicrophonePermission('granted');
      console.log('ðŸŽ¤ Starting audio recording...');
      await audioService.current?.startRecording();
      setIsRecording(true);
      console.log('âœ… Voice recording started');
    } catch (error) {
      console.error('âŒ Failed to start recording:', error);
      setMicrophonePermission('denied');
    }
  }, [connectionStatus, connect]);

  const stopVoiceRecording = useCallback(() => {
    console.log('ðŸ›‘ Stopping voice recording...');
    audioService.current?.stopRecording();
    setIsRecording(false);
    
    if (wsService.current && connectionStatus === 'connected') {
      console.log('ðŸŽ¤ Sending audio stream end signal');
      wsService.current.sendRealtimeInput({ audioStreamEnd: {} });
    }
  }, [connectionStatus]);

  const sendTextMessage = useCallback((message: string) => {
    if (!message.trim() || connectionStatus !== 'connected') {
      console.warn('âš ï¸ Cannot send message - empty or not connected');
      return;
    }
    
    console.log('ðŸ’¬ Sending text message:', message);
    const userMessage: GeminiMessage = {
      id: Date.now().toString(),
      role: 'user',
      content: { text: message },
      timestamp: Date.now()
    };
    setMessages(prev => [...prev, userMessage]);
    
    console.log('ðŸš€ Sending to Gemini Live API via WebSocket...');
    wsService.current?.sendClientContent({
      turns: [{
        role: 'user',
        parts: [{ text: message }]
      }],
      turnComplete: true
    });
  }, [connectionStatus]);

  const playAudioResponse = useCallback(async (audioData: string) => {
    try {
      const binaryString = atob(audioData);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }
      
      const audioContext = new AudioContext();
      const audioBuffer = await audioContext.decodeAudioData(bytes.buffer);
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContext.destination);
      source.start();
    } catch (error) {
      console.error('Failed to play audio response:', error);
    }
  }, []);

  const toggleTextMode = useCallback(() => {
    if (isTextMode && !isSidebarOpen) {
      setIsSidebarOpen(true);
    } else if (isTextMode && isSidebarOpen) {
      setIsSidebarOpen(false);
    } else {
      setIsTextMode(true);
    }
  }, [isTextMode, isSidebarOpen]);

  const toggleSidebar = useCallback(() => {
    setIsSidebarOpen(!isSidebarOpen);
  }, [isSidebarOpen]);

  const closeSidebar = useCallback(() => {
    setIsSidebarOpen(false);
  }, []);

  const backToVoice = useCallback(() => {
    setIsTextMode(false);
    setIsSidebarOpen(false);
  }, []);

  const sendMessage = useCallback((message: string) => {
    sendTextMessage(message);
  }, [sendTextMessage]);

  const value: AIAssistantContextType = {
    connectionStatus,
    sessionId,
    isRecording,
    audioLevel,
    microphonePermission,
    messages,
    isStreaming,
    currentResponse,
    isTextMode,
    isSidebarOpen,
    selectedFilter,
    isLive,
    lastResponse,
    connect,
    disconnect,
    startVoiceRecording,
    stopVoiceRecording,
    sendTextMessage,
    setIsTextMode,
    setIsSidebarOpen,
    setSelectedFilter,
    toggleTextMode,
    toggleSidebar,
    closeSidebar,
    backToVoice,
    sendMessage,
  };

  return (
    <AIAssistantContext.Provider value={value}>
      {children}
    </AIAssistantContext.Provider>
  );
}